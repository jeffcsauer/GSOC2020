{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GSoC Progress Phase II Demonstration Notebook\n",
    "*This notebook demonstrates progress on the GSoC 2020 project entitled [PySAL ESDA Enhancements: Local join count and LOSH statistics](https://docs.google.com/document/d/1WjHjy5Eyk4WG5QWfnsnhWg1r4-e09JXXCx0iaPphg6c/edit).*\n",
    "\n",
    "All proposed estimators have been implemented with docstrings, doctests, and stylized using the PEP8 style guide. A [PR is open](https://github.com/pysal/esda/pull/139) documenting these proposed contributions. A GSOC call on 7/24/2020 outlined several additional tasks to keep the contribution momentum going:\n",
    "\n",
    "- Updating LOSH inference to new the `__crand() engine`\n",
    "- Drafting univariate and multivariate local Geary estimators\n",
    "\n",
    "I've made incremental progress on bullets one and two, but I have yet to start on bullet three. Progress on bullet one and two are demonstrated below. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Updating LOSH inference to new the `__crand() engine`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first import the most up-to-date version of the `losh` function and some other relevant modules. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from esda.losh import losh\n",
    "import numpy as np\n",
    "import geopandas as gpd\n",
    "import libpysal.weights as lp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then run the `losh` function on some trial data, specifically the Denver Housing Dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "denver = gpd.read_file('https://github.com/jeffcsauer/GSOC2020/raw/master/validation/data/denver/denver.gpkg')\n",
    "y_denver = denver['HU_RENTED']\n",
    "# Create weights\n",
    "wq_denver = lp.Queen.from_dataframe(denver)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_losh = losh(connectivity=wq_denver, inference=\"chi-square\").fit(y_denver)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then print out the LOSH $H_i$ values..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.72862202, 0.60570534, 1.27499573, 1.27775056, 3.66426323,\n",
       "       0.20630709, 0.30039163, 3.22878969, 2.47324962, 6.21885436,\n",
       "       4.89325502, 4.93389736, 1.94911049, 0.52298053, 0.55981765,\n",
       "       0.82850515, 1.05341417, 0.84461904, 0.60987348, 0.94527555,\n",
       "       4.26593154, 0.96302429, 1.0423338 , 0.25133172, 0.17036824,\n",
       "       0.42732169, 2.11051377, 1.53535997, 1.23655419, 0.27353575,\n",
       "       0.13635485, 1.83236117, 1.52523996, 0.25140425, 0.30772211,\n",
       "       0.47047662, 0.97574338, 0.09293957, 0.52861436, 0.40511641,\n",
       "       0.56719311, 0.26454396, 0.21123503, 0.15851932, 1.47641653,\n",
       "       0.53764716, 0.52337802, 0.39966269, 0.53409657, 0.04247725,\n",
       "       2.13130198, 0.16518512, 1.18980213, 0.05127779, 5.30026563,\n",
       "       0.60634197, 1.96356402, 1.3692009 , 1.01438307, 0.47150821,\n",
       "       0.13384698, 0.11567471, 0.28335191, 0.65958817, 0.38815222,\n",
       "       0.25514561, 0.21688526, 0.31786797, 0.03497591, 0.41814267,\n",
       "       1.014082  , 0.15674392, 0.23448769, 0.21170868, 0.081058  ])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_losh.Hi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As well as the LOSH $\\chi^2$ p-values..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.43800098, 0.63274819, 0.27976395, 0.27963721, 0.01625474,\n",
       "       0.83193401, 0.82488061, 0.02148205, 0.06894109, 0.00291876,\n",
       "       0.00977939, 0.00569095, 0.1621828 , 0.69043069, 0.55252597,\n",
       "       0.47777914, 0.3427617 , 0.43831955, 0.55728303, 0.39508798,\n",
       "       0.00776009, 0.40896109, 0.34640792, 0.79661973, 0.89210175,\n",
       "       0.70368663, 0.14544505, 0.21350185, 0.29456901, 0.68194318,\n",
       "       0.85051163, 0.17564698, 0.20623918, 0.75327493, 0.5891479 ,\n",
       "       0.70259935, 0.36919055, 0.84387688, 0.29230521, 0.53346689,\n",
       "       0.54853048, 0.78647001, 0.78542879, 0.87055297, 0.22952724,\n",
       "       0.56473036, 0.60826038, 0.72309133, 0.65862816, 0.94533262,\n",
       "       0.14343363, 0.82457905, 0.30101181, 0.97508279, 0.00674329,\n",
       "       0.58648503, 0.13595911, 0.25396347, 0.28256421, 0.60294959,\n",
       "       0.852825  , 0.74525482, 0.8372008 , 0.42308931, 0.65533261,\n",
       "       0.82871472, 0.85737389, 0.65067648, 0.95392359, 0.67596931,\n",
       "       0.36792767, 0.59472128, 0.37904896, 0.65643836, 0.78716864])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_losh.pval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These will serve as a comparison to the values generated in the numba transition. We load in the necessary modules for `_crand()` computation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from esda.crand import (\n",
    "    crand as _crand_plus,\n",
    "    njit as _njit,\n",
    "    _prepare_univariate\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then write some numba-compatible code to calculate $H_i$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "@_njit(fastmath=True)\n",
    "def _losh(i, z, permuted_ids, weights_i, scaling):\n",
    "    zi, zrand = _prepare_univariate(i, z, permuted_ids, weights_i)\n",
    "    \n",
    "    # Working\n",
    "    rowsum = np.sum(weights_i)\n",
    "    # Working\n",
    "    ylag = (zrand @ weights_i)\n",
    "    # Working\n",
    "    yresid = ((zi-ylag)*-1)**2\n",
    "    # Not working exactly? Should be a fixed value?\n",
    "    denom = np.mean(yresid) * rowsum\n",
    "    # Run _prepare_univariate again to get spatial lag of residuals\n",
    "    yresid_i, yresid_rand = _prepare_univariate(i, yresid, permuted_ids, weights_i)\n",
    "    # Run final Hi Calculation\n",
    "    Hi = (yresid_rand @ weights_i) / denom\n",
    "        \n",
    "    return Hi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most of these individual lines are behaving as expected in that they produce similar values to the non-numba code. If we pass the above numba function through `_crand_plus()` we are principally interested in looking at the `rHi` values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.447 0.007 0.044 0.227 0.001 0.001 0.001 0.001 0.003 0.001 0.001 0.001\n",
      " 0.021 0.013 0.141 0.444 0.28  0.43  0.123 0.324 0.001 0.321 0.309 0.001\n",
      " 0.001 0.008 0.003 0.028 0.022 0.143 0.001 0.012 0.07  0.01  0.1   0.003\n",
      " 0.367 0.001 0.404 0.168 0.066 0.001 0.001 0.001 0.103 0.076 0.037 0.002\n",
      " 0.005 0.001 0.024 0.001 0.149 0.001 0.001 0.095 0.001 0.023 0.206 0.093\n",
      " 0.001 0.004 0.001 0.439 0.021 0.001 0.001 0.04  0.001 0.023 0.12  0.346\n",
      " 0.359 0.107 0.043]\n",
      "[[1.25056273 0.70102753 0.27614659 ... 1.73634114 2.36023747 1.26879172]\n",
      " [0.78768735 1.24853582 0.92181837 ... 0.90223035 0.94639712 1.16559047]\n",
      " [0.78794952 1.22519175 0.9157881  ... 0.89658239 0.95294932 1.15697598]\n",
      " ...\n",
      " [0.07719271 1.51467175 0.51201383 ... 1.00240849 0.44995257 1.00240849]\n",
      " [0.82279984 0.199793   0.22131767 ... 1.13951941 0.23211767 0.56425032]\n",
      " [0.76804103 0.45402607 0.13292046 ... 1.11928031 0.11735213 0.50438961]]\n"
     ]
    }
   ],
   "source": [
    "# Note: forcing z and observed into arrays - need to incorporate this into above function?\n",
    "# Standardized: wq_denver\n",
    "# Non-standardized: wq_denver_ns\n",
    "p_sim, rHi = _crand_plus(z=np.array(y_denver), w=wq_denver, observed=np.array(test_losh.Hi), \n",
    "            permutations=999, keep=True, n_jobs=1, \n",
    "            stat_func=_losh)\n",
    "print(p_sim)\n",
    "print(rHi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With each run the above values will change, but we are principally interested in the second set of arrays and the extent to which they align with the `test_losh.Hi` values. They are largely similar to `test_losh.Hi`. The `p_sim` values are irrelevant as they are based on a different calculation.\n",
    "\n",
    "This is where I have paused in the migration as I'm not exactly sure how to proceed. Input is welcome!\n",
    "\n",
    "To my understanding, there are a few options from here. Given these conditionally randomized values, I could simply take the mean and use that mean as the input in the existing chi-square p-value calculation, such as: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.33797649, 0.40640287, 0.41475853, 0.36211762, 0.38267292,\n",
       "       0.36707143, 0.39764617, 0.40814591, 0.42970941, 0.39377397,\n",
       "       0.3884992 , 0.4027361 , 0.3973533 , 0.43637409, 0.39746542,\n",
       "       0.45274732, 0.40004736, 0.44308364, 0.43089687, 0.43593081,\n",
       "       0.45348343, 0.4530607 , 0.38813549, 0.43701048, 0.44518581,\n",
       "       0.45728489, 0.44168431, 0.45253213, 0.44133208, 0.41879683,\n",
       "       0.39423354, 0.40371639, 0.33696717, 0.39530092, 0.388648  ,\n",
       "       0.44903091, 0.39374151, 0.40111497, 0.23921197, 0.38685679,\n",
       "       0.3794951 , 0.40360719, 0.3911361 , 0.42072171, 0.41247489,\n",
       "       0.38819604, 0.4207816 , 0.43804191, 0.44380882, 0.38869435,\n",
       "       0.45279709, 0.40153868, 0.39772761, 0.44021033, 0.45934452,\n",
       "       0.4536327 , 0.46588733, 0.41882124, 0.3582769 , 0.44938826,\n",
       "       0.45176776, 0.43964764, 0.48741048, 0.43042849, 0.44656606,\n",
       "       0.47086906, 0.47753528, 0.42930866, 0.45724734, 0.47346122,\n",
       "       0.47741915, 0.47172698, 0.2948364 , 0.45589874, 0.47758496])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sim = np.transpose(rHi)\n",
    "Hi_sim = sim.mean(axis=0)\n",
    "dof = 2/test_losh.VarHi\n",
    "Zi = (2*Hi_sim)/test_losh.VarHi\n",
    "from scipy import stats\n",
    "pval = 1 - stats.chi2.cdf(Zi, dof)\n",
    "pval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, I'm not sure this is appropriate as it uses the original `VarHi`. This is especially noticeable when comparing the p-values between those from the original (`og`) and conditionally-randomized (`cr`) approach:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.21993687049346328"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "corrdf = pd.DataFrame(test_losh.pval, pval).reset_index()\n",
    "corrdf.columns = ['pval_og', 'pval_cr']\n",
    "corrdf['pval_og'].corr(corrdf['pval_cr'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Attempts to calculate `VarHi` in the `_crand()` engine are proving a bit tricky and deviate quite far from the observed `VarHi` values. Moreover, the above method is quite different from the bootstrap method proposed by [Xu et al 2014](https://link.springer.com/article/10.1007%2Fs00168-014-0605-5) and implmeneted in `R` `spdep::LOSH.mc`. I think I need to more closely examine both the paper and [this section of the `R` code](https://github.com/r-spatial/spdep/blob/master/R/LOSH.mc.R#L71-L86) to understand its implementation. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Local Geary statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While the migration of `losh()` has been a bit rough, the Local Geary statistics are proceeding along quite nicely. I have started a workbook (available [here](https://github.com/jeffcsauer/GSOC2020/blob/master/review/Local_Geary_Workbook.ipynb)) where I work through the calculations and start constructing the functions. Presently the functions are returning correct Local Geary values in both the univariate and multivariate case. However, I have yet to work on inference. I am hoping to tackle that in the first week of August."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Local Geary Univariate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "from scipy import sparse\n",
    "from scipy import stats\n",
    "from sklearn.base import BaseEstimator\n",
    "import libpysal as lp\n",
    "from esda.crand import (\n",
    "    crand as _crand_plus,\n",
    "    njit as _njit,\n",
    "    _prepare_univariate\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "class Local_Geary(BaseEstimator):\n",
    "    \"\"\"Local Geary - Univariate\"\"\"\n",
    "\n",
    "    def __init__(self, connectivity=None, inference=None):\n",
    "        \"\"\"\n",
    "        Initialize a Local_Geary estimator\n",
    "\n",
    "        Arguments\n",
    "        ---------\n",
    "        connectivity     : scipy.sparse matrix object\n",
    "                           the connectivity structure describing the\n",
    "                           relationships between observed units.\n",
    "        inference        : str\n",
    "                           describes type of inference to be used. options are\n",
    "                           \"chi-square\" or \"permutation\" methods.\n",
    "\n",
    "        Attributes\n",
    "        ----------\n",
    "        localG           : numpy array\n",
    "                           Array of Local Geary values for each spatial unit.\n",
    "        pval             : numpy array\n",
    "                           P-values for inference based on either\n",
    "                           \"chi-square\" or \"permutation\" methods.\n",
    "        \"\"\"\n",
    "\n",
    "        self.connectivity = connectivity\n",
    "        self.inference = inference\n",
    "\n",
    "    def fit(self, x):\n",
    "        \"\"\"\n",
    "        Arguments\n",
    "        ---------\n",
    "        x                : numpy.ndarray\n",
    "                           array containing continuous data\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        the fitted estimator.\n",
    "\n",
    "        Notes\n",
    "        -----\n",
    "        Technical details and derivations can be found in :cite:`Anselin1995`.\n",
    "\n",
    "        Examples\n",
    "        --------\n",
    "        Guerry data replication GeoDa tutorial\n",
    "        >>> import libpysal\n",
    "        >>> import geopandas as gpd\n",
    "        >>> guerry = lp.examples.load_example('Guerry')\n",
    "        >>> guerry_ds = gpd.read_file(guerry.get_path('Guerry.shp'))\n",
    "        >>> w = libpysal.weights.Queen.from_dataframe(guerry_ds)\n",
    "        \"\"\"\n",
    "        x = np.asarray(x).flatten()\n",
    "\n",
    "        w = self.connectivity\n",
    "        w.transform = 'r'\n",
    "\n",
    "        self.localG = self._statistic(x, w)\n",
    "\n",
    "        if self.inference is None:\n",
    "        #   self.p_sim, self.rjoins = _crand_plus(\n",
    "        #       z=self.x, \n",
    "        #       w=self.w, \n",
    "        #       observed=self.localG,\n",
    "        #       permutations=permutations, \n",
    "        #       keep=True, \n",
    "        #       n_jobs=n_jobs,\n",
    "        #       stat_func=_local_geary\n",
    "        #   )\n",
    "        #   \n",
    "            print(\"No inference selected.\")\n",
    "        else:\n",
    "            raise NotImplementedError(f'The requested inference method \\\n",
    "            ({self.inference}) is not currently supported!')\n",
    "\n",
    "        return self\n",
    "\n",
    "    @staticmethod\n",
    "    def _statistic(x, w):\n",
    "        # Caclulate z-scores for x\n",
    "        zscore_x = (x - np.mean(x))/np.std(x)\n",
    "        # Create focal (xi) and neighbor (zi) values\n",
    "        adj_list = w.to_adjlist(remove_symmetric=False)\n",
    "        zseries = pd.Series(zscore_x, index=wq.id_order)\n",
    "        zi = zseries.loc[adj_list.focal].values\n",
    "        zj = zseries.loc[adj_list.neighbor].values\n",
    "        # Carry out local Geary calculation\n",
    "        gs = sum(list(wq.weights.values()), []) * (zi-zj)**2\n",
    "        # Reorganize data\n",
    "        adj_list_gs = pd.DataFrame(adj_list.focal.values, gs).reset_index()\n",
    "        adj_list_gs.columns = ['gs', 'ID']\n",
    "        adj_list_gs = adj_list_gs.groupby(by='ID').sum()\n",
    "        \n",
    "        localG = adj_list_gs.gs.values\n",
    "        \n",
    "        return (localG)\n",
    "\n",
    "# --------------------------------------------------------------\n",
    "# Conditional Randomization Function Implementations\n",
    "# --------------------------------------------------------------\n",
    "\n",
    "@_njit(fastmath=True)\n",
    "def _local_geary(i, z, permuted_ids, weights_i, scaling):\n",
    "    zi, zrand = _prepare_univariate(i, z, permuted_ids, weights_i)\n",
    "    return zi * (zrand @ weights_i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following the GeoDa web example, we can apply the `Local_Geary` function on the `Donatns` column of the `Guerry` dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import libpysal as lp\n",
    "import geopandas as gpd\n",
    "from scipy import stats\n",
    "import numpy as np\n",
    "guerry = lp.examples.load_example('Guerry')\n",
    "guerry_ds = gpd.read_file(guerry.get_path('Guerry.shp'))\n",
    "wq = lp.weights.Queen.from_dataframe(guerry_ds)\n",
    "x = guerry_ds['Donatns']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No inference selected.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([1.82087039e-01, 5.60014026e-01, 9.75294606e-01, 2.15906938e-01,\n",
       "       6.17372564e-01, 3.84450059e-02, 2.43181756e-01, 9.71802819e-01,\n",
       "       4.06447101e-02, 7.24722785e-01, 6.30952854e-02, 2.42104497e-02,\n",
       "       1.59496916e+01, 9.29326006e-01, 9.65188634e-01, 1.32383286e+00,\n",
       "       3.31775497e-01, 2.99446505e+00, 9.43946814e-01, 2.99570159e+00,\n",
       "       3.66702291e-01, 2.09592365e+00, 1.46515861e+00, 1.82118455e-01,\n",
       "       3.10216680e+00, 5.43063937e-01, 5.74532559e+00, 4.79160197e-02,\n",
       "       1.58993089e-01, 7.18327253e-01, 1.24297849e+00, 8.72629331e-02,\n",
       "       7.52809650e-01, 4.56515485e-01, 3.86766562e-01, 1.17632604e-01,\n",
       "       6.90884685e-01, 2.87206102e+00, 4.10455112e-01, 4.04349959e-01,\n",
       "       1.14211758e-01, 9.59519953e-01, 3.51347976e-01, 7.30240974e-01,\n",
       "       4.40370938e-01, 7.20360356e-02, 1.66241706e+00, 5.83258909e+00,\n",
       "       2.30332507e-01, 4.38369688e-01, 8.41461470e-01, 1.52959486e+00,\n",
       "       4.32157479e-02, 2.08325903e+00, 1.19722984e+00, 1.28169257e+00,\n",
       "       1.32443562e-01, 3.97452281e-01, 3.39175467e+00, 1.55325208e-02,\n",
       "       1.08504192e+00, 1.20141728e+00, 4.42727015e-01, 3.16097426e+00,\n",
       "       3.38714881e+00, 1.61732976e+00, 2.05483294e-01, 2.95666506e+00,\n",
       "       6.42017171e-01, 4.21213806e-01, 2.66678544e-02, 4.02155064e-01,\n",
       "       1.17112566e-01, 4.57596388e-01, 2.18572511e+00, 1.95661321e-01,\n",
       "       5.57923492e-02, 8.23656724e-02, 2.20496135e-02, 1.02137241e-01,\n",
       "       1.43925735e+00, 9.93140340e-01, 8.76795695e-01, 1.22224557e+00,\n",
       "       3.66964823e-01])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "functest = Local_Geary(connectivity=wq).fit(x)\n",
    "functest.localG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Local Geary Multivariate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "from scipy import sparse\n",
    "from scipy import stats\n",
    "from sklearn.base import BaseEstimator\n",
    "import libpysal as lp\n",
    "\n",
    "\n",
    "class Local_Geary_MV(BaseEstimator):\n",
    "    \"\"\"Local Geary - Univariate\"\"\"\n",
    "\n",
    "    def __init__(self, connectivity=None, inference=None):\n",
    "        \"\"\"\n",
    "        Initialize a Local_Geary estimator\n",
    "\n",
    "        Arguments\n",
    "        ---------\n",
    "        connectivity     : scipy.sparse matrix object\n",
    "                           the connectivity structure describing the\n",
    "                           relationships between observed units.\n",
    "        inference        : str\n",
    "                           describes type of inference to be used. options are\n",
    "                           \"chi-square\" or \"permutation\" methods.\n",
    "\n",
    "        Attributes\n",
    "        ----------\n",
    "        localG           : numpy array\n",
    "                           Array of Local Geary values for each spatial unit.\n",
    "        pval             : numpy array\n",
    "                           P-values for inference based on either\n",
    "                           \"chi-square\" or \"permutation\" methods.\n",
    "        \"\"\"\n",
    "\n",
    "        self.connectivity = connectivity\n",
    "        self.inference = inference\n",
    "\n",
    "    def fit(self, variables):\n",
    "        \"\"\"\n",
    "        Arguments\n",
    "        ---------\n",
    "        variables        : numpy.ndarray\n",
    "                           array containing continuous data\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        the fitted estimator.\n",
    "\n",
    "        Notes\n",
    "        -----\n",
    "        Technical details and derivations can be found in :cite:`Anselin1995`.\n",
    "\n",
    "        Examples\n",
    "        --------\n",
    "        Guerry data replication GeoDa tutorial\n",
    "        >>> import libpysal\n",
    "        >>> import geopandas as gpd\n",
    "        >>> guerry = lp.examples.load_example('Guerry')\n",
    "        >>> guerry_ds = gpd.read_file(guerry.get_path('Guerry.shp'))\n",
    "        >>> w = libpysal.weights.Queen.from_dataframe(guerry_ds)\n",
    "        \"\"\"\n",
    "        self.variables = np.array(variables, dtype='float')\n",
    "\n",
    "        w = self.connectivity\n",
    "        w.transform = 'r'\n",
    "\n",
    "        self.localG = self._statistic(variables, w)\n",
    "\n",
    "        if self.inference is None:\n",
    "            return self\n",
    "        #elif self.inference == 'chi-square':\n",
    "        #    if a != 2:\n",
    "        #        warnings.warn(f'Chi-square inference assumes that a=2, but \\\n",
    "        #        a={a}. This means the inference will be invalid!')\n",
    "        #    else:\n",
    "        #        dof = 2/self.VarHi\n",
    "        #        Zi = (2*self.Hi)/self.VarHi\n",
    "        #        self.pval = 1 - stats.chi2.cdf(Zi, dof)\n",
    "        else:\n",
    "            raise NotImplementedError(f'The requested inference method \\\n",
    "            ({self.inference}) is not currently supported!')\n",
    "\n",
    "        return self\n",
    "\n",
    "    @staticmethod\n",
    "    def _statistic(variables, w):\n",
    "        # Caclulate z-scores for input variables\n",
    "        zseries = [stats.zscore(i) for i in variables]\n",
    "        # Define denominator adjustment\n",
    "        k = len(variables)\n",
    "        # Create focal and neighbor values\n",
    "        adj_list = w.to_adjlist(remove_symmetric=False)\n",
    "        zseries = [pd.Series(i, index=wq.id_order) for i in zseries]\n",
    "        focal = [zseries[i].loc[adj_list.focal].values for\n",
    "                 i in range(len(variables))]\n",
    "        neighbor = [zseries[i].loc[adj_list.neighbor].values for\n",
    "                    i in range(len(variables))]\n",
    "        # Carry out local Geary calculation\n",
    "        gs = sum(list(wq.weights.values()), []) * \\\n",
    "        (np.array(focal) - np.array(neighbor))**2\n",
    "        # Reorganize data\n",
    "        temp = pd.DataFrame(gs).T\n",
    "        temp['ID'] = adj_list.focal.values\n",
    "        adj_list_gs = temp.groupby(by='ID').sum()\n",
    "        localG = adj_list_gs.sum(axis=1)/k\n",
    "        \n",
    "        return (localG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As before, we can apply this on `Donatns` and `Suicids`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = guerry_ds['Donatns']\n",
    "y = guerry_ds['Suicids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ID\n",
       "0     0.153819\n",
       "1     0.303560\n",
       "2     2.954720\n",
       "3     0.123140\n",
       "4     0.387960\n",
       "        ...   \n",
       "80    1.657430\n",
       "81    0.525764\n",
       "82    0.645337\n",
       "83    0.717948\n",
       "84    0.216181\n",
       "Length: 85, dtype: float64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "functest = Local_Geary_MV(connectivity=wq).fit([x,y])\n",
    "functest.localG"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
