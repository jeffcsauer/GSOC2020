{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Purpose of this notebook\n",
    "\n",
    "The purpose of this notebook is to migrate the workbook pseudo code of `LOSH_*.ipynb` and `OLJC_*.ipynb` into functions that match the `PySAL` structure. These will be expanded over time and built out."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example function from pysal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Spatial autocorrelation for binary attributes\n",
    "\"\"\"\n",
    "__author__ = \"Sergio J. Rey <srey@asu.edu> , Luc Anselin <luc.anselin@asu.edu>\"\n",
    "\n",
    "from libpysal.weights.spatial_lag import lag_spatial\n",
    "from esda.tabular import _univariate_handler # change from .tabular to esda.tabular when working on independent machine\n",
    "from scipy.stats import chi2_contingency\n",
    "from scipy.stats import chi2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "__all__ = ['Join_Counts']\n",
    "\n",
    "PERMUTATIONS = 999\n",
    "\n",
    "class Join_Counts(object):\n",
    "    \"\"\"Binary Join Counts\n",
    "    Parameters\n",
    "    ----------\n",
    "    y               : array\n",
    "                      binary variable measured across n spatial units\n",
    "    w               : W\n",
    "                      spatial weights instance\n",
    "    permutations    : int\n",
    "                      number of random permutations for calculation of pseudo-p_values\n",
    "    Attributes\n",
    "    ----------\n",
    "    y            : array\n",
    "                   original variable\n",
    "    w            : W\n",
    "                   original w object\n",
    "    permutations : int\n",
    "                   number of permutations\n",
    "    bb           : float\n",
    "                   number of black-black joins\n",
    "    ww           : float\n",
    "                   number of white-white joins\n",
    "    bw           : float\n",
    "                   number of black-white joins\n",
    "    J            : float\n",
    "                   number of joins\n",
    "    sim_bb       : array\n",
    "                   (if permutations>0)\n",
    "                   vector of bb values for permuted samples\n",
    "    p_sim_bb     : array\n",
    "                  (if permutations>0)\n",
    "                   p-value based on permutations (one-sided)\n",
    "                   null: spatial randomness\n",
    "                   alternative: the observed bb is greater than under randomness\n",
    "    mean_bb      : float\n",
    "                   average of permuted bb values\n",
    "    min_bb       : float\n",
    "                   minimum of permuted bb values\n",
    "    max_bb       : float\n",
    "                   maximum of permuted bb values\n",
    "    sim_bw       : array\n",
    "                   (if permutations>0)\n",
    "                   vector of bw values for permuted samples\n",
    "    p_sim_bw     : array\n",
    "                   (if permutations>0)\n",
    "                   p-value based on permutations (one-sided)\n",
    "                   null: spatial randomness\n",
    "                   alternative: the observed bw is greater than under randomness\n",
    "    mean_bw      : float\n",
    "                   average of permuted bw values\n",
    "    min_bw       : float\n",
    "                   minimum of permuted bw values\n",
    "    max_bw       : float\n",
    "                   maximum of permuted bw values\n",
    "    chi2         : float\n",
    "                   Chi-square statistic on contingency table for join counts\n",
    "    chi2_p       : float\n",
    "                   Analytical p-value for chi2\n",
    "    chi2_dof     : int\n",
    "                   Degrees of freedom for analytical chi2\n",
    "    crosstab     : DataFrame\n",
    "                   Contingency table for observed join counts\n",
    "    expected     : DataFrame\n",
    "                   Expected contingency table for the null \n",
    "    p_sim_chi2   : float\n",
    "                   p-value for chi2 under random spatial permutations\n",
    "    Examples\n",
    "    --------\n",
    "    >>> import numpy as np\n",
    "    >>> import libpysal\n",
    "    >>> w = libpysal.weights.lat2W(4, 4)\n",
    "    >>> y = np.ones(16)\n",
    "    >>> y[0:8] = 0\n",
    "    >>> np.random.seed(12345)\n",
    "    >>> from esda.join_counts import Join_Counts\n",
    "    >>> jc = Join_Counts(y, w)\n",
    "    >>> jc.bb\n",
    "    10.0\n",
    "    >>> jc.bw\n",
    "    4.0\n",
    "    >>> jc.ww\n",
    "    10.0\n",
    "    >>> jc.J\n",
    "    24.0\n",
    "    >>> len(jc.sim_bb)\n",
    "    999\n",
    "    >>> round(jc.p_sim_bb, 3)\n",
    "    0.003\n",
    "    >>> round(np.mean(jc.sim_bb), 3)\n",
    "    5.547\n",
    "    >>> np.max(jc.sim_bb)\n",
    "    10.0\n",
    "    >>> np.min(jc.sim_bb)\n",
    "    0.0\n",
    "    >>> len(jc.sim_bw)\n",
    "    999\n",
    "    >>> jc.p_sim_bw\n",
    "    1.0\n",
    "    >>> np.mean(jc.sim_bw)\n",
    "    12.811811811811811\n",
    "    >>> np.max(jc.sim_bw)\n",
    "    24.0\n",
    "    >>> np.min(jc.sim_bw)\n",
    "    7.0\n",
    "    >>> round(jc.chi2_p, 3)\n",
    "    0.004\n",
    "    >>> jc.p_sim_chi2\n",
    "    0.002\n",
    "    Notes\n",
    "    -----\n",
    "    Technical details and derivations can be found in :cite:`cliff81`.\n",
    "    \"\"\"\n",
    "    def __init__(self, y, w, permutations=PERMUTATIONS):\n",
    "        y = np.asarray(y).flatten()\n",
    "        w.transformation = 'b'  # ensure we have binary weights\n",
    "        self.w = w\n",
    "        self.adj_list = self.w.to_adjlist(remove_symmetric=True) # a function of the weights method in pysal, \n",
    "        # to_adjlist(self[, remove_symmetric, …]) which computes an adjacency list representation of a weights object.\n",
    "        self.y = y\n",
    "        self.permutations = permutations\n",
    "        self.J = w.s0 / 2.\n",
    "        results = self.__calc(self.y)\n",
    "        self.bb = results[0]\n",
    "        self.ww = results[1]\n",
    "        self.bw = results[2]\n",
    "        self.chi2 = results[3]\n",
    "        self.chi2_p = results[4]\n",
    "        self.chi2_dof = results[5]\n",
    "        self.autocorr_pos = self.bb + self.ww\n",
    "        self.autocorr_neg = self.bw\n",
    "\n",
    "        crosstab = pd.DataFrame(data=results[-1])\n",
    "        id_names = ['W', 'B']\n",
    "        idx = pd.Index(id_names, name='Focal')\n",
    "        crosstab.set_index(idx, inplace=True)\n",
    "        crosstab.columns = pd.Index(id_names, name='Neighbor')\n",
    "        self.crosstab = crosstab\n",
    "        expected = pd.DataFrame(data=results[6])\n",
    "        expected.set_index(idx, inplace=True)\n",
    "        expected.columns = pd.Index(id_names, name='Neighbor')\n",
    "        self.expected = expected\n",
    "        self.calc = self.__calc\n",
    "\n",
    "        if permutations:\n",
    "            sim = []\n",
    "            i = 0\n",
    "            while i < permutations:\n",
    "                try:\n",
    "                    res = self.__calc(np.random.permutation(self.y))\n",
    "                    sim.append(res)\n",
    "                    i += 1\n",
    "                except ValueError:\n",
    "                    # expected count of 0 -> inadmissible\n",
    "                    pass\n",
    "            sim_jc = np.array(sim, dtype=object)\n",
    "            self.sim_bb = sim_jc[:, 0]\n",
    "            self.min_bb = np.min(self.sim_bb)\n",
    "            self.mean_bb = np.mean(self.sim_bb)\n",
    "            self.max_bb = np.max(self.sim_bb)\n",
    "            self.sim_bw = sim_jc[:, 2]\n",
    "            self.min_bw = np.min(self.sim_bw)\n",
    "            self.mean_bw = np.mean(self.sim_bw)\n",
    "            self.max_bw = np.max(self.sim_bw)\n",
    "            self.sim_autocurr_pos = sim_jc[:, 0]+sim_jc[:, 1]\n",
    "            self.sim_autocurr_neg = sim_jc[:, 2]\n",
    "            self.sim_chi2 = sim_jc[:, 3]\n",
    "\n",
    "            stat = ((self.autocorr_pos - np.mean(self.sim_autocurr_pos))**2 / np.mean(self.sim_autocurr_pos)**2 +\n",
    "                                              (self.autocorr_neg - np.mean(self.sim_autocurr_neg))**2 / np.mean(self.sim_autocurr_pos)**2)\n",
    "            self.sim_autocorr_chi2 = 1 - chi2.cdf(stat, 1)\n",
    "\n",
    "            p_sim_bb = self.__pseudop(self.sim_bb, self.bb)\n",
    "            p_sim_bw = self.__pseudop(self.sim_bw, self.bw)\n",
    "            p_sim_chi2 = self.__pseudop(self.sim_chi2, self.chi2)\n",
    "            p_sim_autocorr_pos = self.__pseudop(self.sim_autocurr_pos, self.autocorr_pos)\n",
    "            p_sim_autocorr_neg = self.__pseudop(self.sim_autocurr_neg, self.autocorr_neg)\n",
    "            self.p_sim_bb = p_sim_bb\n",
    "            self.p_sim_bw = p_sim_bw\n",
    "            self.p_sim_chi2 = p_sim_chi2\n",
    "            self.p_sim_autocorr_pos = p_sim_autocorr_pos\n",
    "            self.p_sim_autocorr_neg = p_sim_autocorr_neg\n",
    "\n",
    "    def __calc(self, z):\n",
    "        adj_list = self.adj_list\n",
    "        zseries = pd.Series(z, index=self.w.id_order)\n",
    "        focal = zseries.loc[adj_list.focal].values\n",
    "        neighbor = zseries.loc[adj_list.neighbor].values\n",
    "        sim = focal == neighbor\n",
    "        dif = 1 - sim\n",
    "        bb = (focal * sim).sum()\n",
    "        ww = ((1-focal) * sim).sum()\n",
    "        bw = (focal * dif).sum()\n",
    "        wb = ((1-focal) * dif).sum()\n",
    "        table = [[ww, wb],\n",
    "                [bw, bb]]\n",
    "        chi2 = chi2_contingency(table)\n",
    "        stat, pvalue, dof, expected = chi2\n",
    "        return (bb, ww, bw+wb, stat, pvalue, dof, expected, np.array(table))\n",
    "\n",
    "    def __pseudop(self, sim, jc):\n",
    "        above = sim >=jc\n",
    "        larger = sum(above)\n",
    "        psim = (larger + 1.) / (self.permutations + 1.)\n",
    "        return psim\n",
    "\n",
    "    @property\n",
    "    def _statistic(self):\n",
    "        return self.bw\n",
    "\n",
    "    @classmethod\n",
    "    def by_col(cls, df, cols, w=None, inplace=False, pvalue='sim', outvals=None, **stat_kws):\n",
    "        \"\"\"\n",
    "        Function to compute a Join_Count statistic on a dataframe\n",
    "        Arguments\n",
    "        ---------\n",
    "        df          :   pandas.DataFrame\n",
    "                        a pandas dataframe with a geometry column\n",
    "        cols        :   string or list of string\n",
    "                        name or list of names of columns to use to compute the statistic\n",
    "        w           :   pysal weights object\n",
    "                        a weights object aligned with the dataframe. If not provided, this\n",
    "                        is searched for in the dataframe's metadata\n",
    "        inplace     :   bool\n",
    "                        a boolean denoting whether to operate on the dataframe inplace or to\n",
    "                        return a series contaning the results of the computation. If\n",
    "                        operating inplace, the derived columns will be named\n",
    "                        'column_join_count'\n",
    "        pvalue      :   string\n",
    "                        a string denoting which pvalue should be returned. Refer to the\n",
    "                        the Join_Count statistic's documentation for available p-values\n",
    "        outvals     :   list of strings\n",
    "                        list of arbitrary attributes to return as columns from the\n",
    "                        Join_Count statistic\n",
    "        **stat_kws  :   keyword arguments\n",
    "                        options to pass to the underlying statistic. For this, see the\n",
    "                        documentation for the Join_Count statistic.\n",
    "        Returns\n",
    "        --------\n",
    "        If inplace, None, and operation is conducted on dataframe in memory. Otherwise,\n",
    "        returns a copy of the dataframe with the relevant columns attached.\n",
    "        \"\"\"\n",
    "        if outvals is None:\n",
    "            outvals = []\n",
    "            outvals.extend(['bb', 'p_sim_bw', 'p_sim_bb'])\n",
    "            pvalue = ''\n",
    "        return _univariate_handler(df, cols, w=w, inplace=inplace, pvalue=pvalue,\n",
    "                                   outvals=outvals, stat=cls,\n",
    "                                   swapname='bw', **stat_kws)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LJC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Univariate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Spatial autocorrelation for binary attributes\n",
    "\"\"\"\n",
    "\n",
    "__author__ = \"Sergio J. Rey <srey@asu.edu> , Luc Anselin <luc.anselin@asu.edu>\"\n",
    "\n",
    "from libpysal.weights.spatial_lag import lag_spatial\n",
    "# from esda.tabular import _univariate_handler # don't need in my functions at the moment - if in df then yes!\n",
    "#from scipy.stats import chi2_contingency\n",
    "#from scipy.stats import chi2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "__all__ = ['Join_Counts_Local', # assumed univariate\n",
    "           'Join_Counts_BV', # assumed bivariate\n",
    "           'Join_Counts_MV' # assumed multivariate\n",
    "          ]\n",
    "\n",
    "# PERMUTATIONS = 999\n",
    "\n",
    "class Join_Counts_Local_old(object):\n",
    "    \"\"\"Univariate Local Join Counts\n",
    "    Parameters\n",
    "    ----------\n",
    "    y               : array\n",
    "                      binary variable measured across n spatial units\n",
    "    w               : W\n",
    "                      spatial weights instance\n",
    "    permutations    : int\n",
    "                      number of random permutations for calculation of pseudo-p_values\n",
    "    Attributes\n",
    "    ----------\n",
    "    y            : array\n",
    "                   original variable\n",
    "    w            : W\n",
    "                   original w object\n",
    "    permutations : int\n",
    "                   number of permutations\n",
    "    bb           : float\n",
    "                   number of black-black joins\n",
    "    J            : float\n",
    "                   number of joins\n",
    "    sim_bb       : array\n",
    "                   (if permutations>0)\n",
    "                   vector of bb values for permuted samples\n",
    "    p_sim_bb     : array\n",
    "                  (if permutations>0)\n",
    "                   p-value based on permutations (one-sided)\n",
    "                   null: spatial randomness\n",
    "                   alternative: the observed bb is greater than under randomness\n",
    "    mean_bb      : float\n",
    "                   average of permuted bb values\n",
    "    min_bb       : float\n",
    "                   minimum of permuted bb values\n",
    "    max_bb       : float\n",
    "                   maximum of permuted bb values\n",
    "    chi2         : float\n",
    "                   Chi-square statistic on contingency table for join counts\n",
    "    chi2_p       : float\n",
    "                   Analytical p-value for chi2\n",
    "    chi2_dof     : int\n",
    "                   Degrees of freedom for analytical chi2\n",
    "    crosstab     : DataFrame\n",
    "                   Contingency table for observed join counts\n",
    "    expected     : DataFrame\n",
    "                   Expected contingency table for the null \n",
    "    p_sim_chi2   : float\n",
    "                   p-value for chi2 under random spatial permutations\n",
    "    Notes\n",
    "    -----\n",
    "    Technical details and derivations can be found in :cite:`anselinli2019`.\n",
    "    \"\"\"\n",
    "    def __init__(self, y, w):\n",
    "        y = np.asarray(y).flatten()\n",
    "        w.transformation = 'b'  # ensure we have binary weights\n",
    "        self.w = w\n",
    "        self.adj_list = self.w.to_adjlist(remove_symmetric=False) # this differs from esda.Join_Counts() function\n",
    "        self.y = y\n",
    "        #self.permutations = permutations\n",
    "        #self.J = w.s0 / 2.\n",
    "        results = self.__calc(self.y)\n",
    "        self.bb = results # as there is only one item being returned right now, \n",
    "                          # we just use results. once more things are returned in last line of __calc this should return to results[0]\n",
    "        #self.chi2 = results[3]\n",
    "        #self.chi2_p = results[4]\n",
    "        #self.chi2_dof = results[5]\n",
    "        #self.autocorr_pos = self.bb + self.ww\n",
    "        #self.autocorr_neg = self.bw\n",
    "    \n",
    "    def __calc(self, z):\n",
    "        adj_list = self.adj_list\n",
    "        zseries = pd.Series(z, index=self.w.id_order)\n",
    "        focal = zseries.loc[adj_list.focal].values\n",
    "        neighbor = zseries.loc[adj_list.neighbor].values\n",
    "        BB = (focal == 1) & (neighbor == 1)\n",
    "        adj_list_BB = pd.DataFrame(adj_list.focal.values, BB.astype('uint8')).reset_index()\n",
    "        adj_list_BB.columns = ['BB', 'ID']\n",
    "        adj_list_BB = adj_list_BB.groupby(by='ID').sum()\n",
    "        BB = adj_list_BB.BB.values\n",
    "        #print(BB)\n",
    "        return (BB)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above function is working but is in the 'old' `moran.py` or `join_counts.py` formatting style. Levi suggested making them in the form of scikit-learn or scipy. I'm leaning torwards the scikit-learn style and so I'm emulating `lee.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/pysal/esda/blob/master/esda/lee.py\n",
    "import numpy\n",
    "from scipy import sparse\n",
    "from sklearn.base import BaseEstimator\n",
    "from sklearn import preprocessing\n",
    "from sklearn import utils\n",
    "\n",
    "class Join_Counts_Local(BaseEstimator):\n",
    "    \"\"\"Global Spatial Pearson Statistic\"\"\"\n",
    "\n",
    "    def __init__(self, connectivity=None):\n",
    "        \"\"\"\n",
    "        Initialize a Join_Counts_Local estimator\n",
    "        Arguments\n",
    "        ---------\n",
    "        connectivity:   scipy.sparse matrix object\n",
    "                        the connectivity structure describing the relationships\n",
    "                        between observed units. Will be row-standardized. \n",
    "        permutations:   int\n",
    "                        the number of permutations to conduct for inference.\n",
    "                        if < 1, no permutational inference will be conducted. \n",
    "        Attributes\n",
    "        ----------\n",
    "        association_: numpy.ndarray (2,2)\n",
    "                      array containg the estimated Lee spatial pearson correlation\n",
    "                      coefficients, where element [0,1] is the spatial correlation\n",
    "                      coefficient, and elements [0,0] and [1,1] are the \"spatial\n",
    "                      smoothing factor\"\n",
    "        reference_distribution_: numpy.ndarray (n_permutations, 2,2)\n",
    "                      distribution of correlation matrices for randomly-shuffled\n",
    "                      maps. \n",
    "        significance_: numpy.ndarray (2,2)\n",
    "                       permutation-based p-values for the fraction of times the\n",
    "                       observed correlation was more extreme than the simulated \n",
    "                       correlations.\n",
    "        \"\"\"\n",
    "        self.connectivity = connectivity\n",
    "        self.permutations = permutations\n",
    "\n",
    "    def fit(self, x, y):\n",
    "        \"\"\"\n",
    "        bivariate spatial pearson's R based on Eq. 18 of :cite:`Lee2001`.\n",
    "        L = \\dfrac{Z^T (V^TV) Z}{1^T (V^TV) 1}\n",
    "        Arguments\n",
    "        ---------\n",
    "        x       :   numpy.ndarray\n",
    "                    array containing continuous data\n",
    "        y       :   numpy.ndarray\n",
    "                    array containing continuous data\n",
    "        Returns\n",
    "        -------\n",
    "        the fitted estimator.\n",
    "        Notes\n",
    "        -----\n",
    "        Technical details and derivations can be found in :cite:`Lee2001`.\n",
    "        \"\"\"\n",
    "        x = utils.check_array(x)\n",
    "        y = utils.check_array(y)\n",
    "        Z = numpy.column_stack((preprocessing.StandardScaler().fit_transform(x),\n",
    "                                preprocessing.StandardScaler().fit_transform(y)))\n",
    "        if self.connectivity is None:\n",
    "            self.connectivity = sparse.eye(Z.shape[0])\n",
    "        self.association_ = self._statistic(Z, self.connectivity) \n",
    "        \n",
    "        standard_connectivity = sparse.csc_matrix(self.connectivity /\n",
    "                                                  self.connectivity.sum(axis=1))\n",
    "\n",
    "        if (self.permutations is None):\n",
    "            return self\n",
    "        elif self.permutations < 1:\n",
    "            return self\n",
    "\n",
    "        if self.permutations:\n",
    "            simulations = [self._statistic(numpy.random.permutation(Z), self.connectivity)\n",
    "                           for _ in range(self.permutations)]\n",
    "            self.reference_distribution_ = simulations = numpy.array(simulations)\n",
    "            above = simulations >= self.association_\n",
    "            larger = above.sum(axis=0)\n",
    "            extreme = numpy.minimum(self.permutations - larger, larger)\n",
    "            self.significance_ = (extreme + 1.) / (self.permutations + 1.)\n",
    "        return self\n",
    "\n",
    "    @staticmethod\n",
    "    def _statistic(Z,W):\n",
    "        ctc = W.T @ W\n",
    "        ones = numpy.ones(ctc.shape[0])\n",
    "        return (Z.T @ ctc @ Z) / (ones.T @ ctc @ ones)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    import geopandas\n",
    "    import libpysal\n",
    "    df = geopandas.read_file(libpysal.examples.get_path('columbus.shp'))\n",
    "    x = df[['HOVAL']].values\n",
    "    y = df[['CRIME']].values\n",
    "    zx = preprocessing.StandardScaler().fit_transform(x)\n",
    "    zy = preprocessing.StandardScaler().fit_transform(y)\n",
    "    w = libpysal.weights.Queen.from_dataframe(df)\n",
    "    w.transform = 'r'\n",
    "    numpy.random.seed(2478879)\n",
    "    testglobal = Spatial_Pearson(connectivity=w.sparse).fit(x,y)\n",
    "    testglobal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.30136527, -0.23625603],\n",
       "       [-0.23625603,  0.53512008]])"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testglobal.association_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/pysal/esda/blob/master/esda/lee.py\n",
    "import numpy\n",
    "from scipy import sparse\n",
    "from sklearn.base import BaseEstimator\n",
    "from sklearn import utils\n",
    "\n",
    "class Local_Join_Count(BaseEstimator):\n",
    "    \"\"\"Local Join Count Statistic\"\"\"\n",
    "\n",
    "    def __init__(self, connectivity=None):\n",
    "        \"\"\"\n",
    "        Initialize a Join_Counts_Local estimator\n",
    "        Arguments\n",
    "        ---------\n",
    "        connectivity:   scipy.sparse matrix object\n",
    "                        the connectivity structure describing the relationships\n",
    "                        between observed units. Will be row-standardized. \n",
    "        Attributes\n",
    "        ----------\n",
    "        BB_:  numpy.ndarray (1,)\n",
    "              array containing the estimated Local Join Count coefficients, \n",
    "              where element [0,0] is the number of Local Join Counts, ...\n",
    "        \"\"\"\n",
    "        \n",
    "        self.connectivity = connectivity\n",
    "\n",
    "    def fit(self, y):\n",
    "        \"\"\"\n",
    "        Arguments\n",
    "        ---------\n",
    "        y       :   numpy.ndarray\n",
    "                    array containing binary (0/1) data\n",
    "        Returns\n",
    "        -------\n",
    "        the fitted estimator.\n",
    "        Notes\n",
    "        -----\n",
    "        Technical details and derivations can be found in :cite:`AnselinLi2019`.\n",
    "        \"\"\"\n",
    "        y = np.asarray(y).flatten()\n",
    "        \n",
    "        w = self.connectivity\n",
    "        w.transformation = 'b' # Ensure we have binary weights   \n",
    "        \n",
    "        self.BB_ = self._statistic(y, w) # Calculate the statistic\n",
    "        \n",
    "        # Need the >>> return self to get the associated .BB_ attribute (as well as significance in future, i.e. self.reference_distribution_ in lee.py)\n",
    "        return self\n",
    "        \n",
    "    @staticmethod\n",
    "    def _statistic(y, w):\n",
    "        adj_list = w.to_adjlist(remove_symmetric=False) # remove_symmetric=False differs from esda.Join_Counts() function\n",
    "        zseries = pd.Series(y, index=w.id_order)\n",
    "        focal = zseries.loc[adj_list.focal].values\n",
    "        neighbor = zseries.loc[adj_list.neighbor].values\n",
    "        BB = (focal == 1) & (neighbor == 1)\n",
    "        adj_list_BB = pd.DataFrame(adj_list.focal.values, BB.astype('uint8')).reset_index()\n",
    "        adj_list_BB.columns = ['BB', 'ID']\n",
    "        adj_list_BB = adj_list_BB.groupby(by='ID').sum()\n",
    "        BB = adj_list_BB.BB.values\n",
    "        return (BB)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test both the old and new function with some inputs..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new y_1 [0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import libpysal\n",
    "import pandas as pd\n",
    "# Create a 16x16 grid\n",
    "w = libpysal.weights.lat2W(4, 4)\n",
    "y_1 = np.ones(16)\n",
    "# Set the first 9 of the ones to 0\n",
    "y_1[0:8] = 0\n",
    "print('new y_1', y_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.Join_Counts_Local_old at 0x1ba21898>"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Join_Counts_Local_old(y_1, w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 0 0 0 0 2 3 3 2 2 3 3 2]\n"
     ]
    }
   ],
   "source": [
    "test_ljc_uni = Join_Counts_Local_old(y_1, w)\n",
    "vars(test_ljc_uni)\n",
    "print(test_ljc_uni.bb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 2, 3, 3, 2, 2, 3, 3, 2], dtype=uint64)"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp = Local_Join_Count(connectivity=w).fit(y_1)\n",
    "temp.BB_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test to ensure equivalency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True])"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_ljc_uni.bb == temp.BB_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created `%t` as an alias for `%timeit`.\n",
      "Created `%%t` as an alias for `%%timeit`.\n"
     ]
    }
   ],
   "source": [
    "# Compare speed of two functions\n",
    "%alias_magic t timeit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.9 ms ± 214 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%t Local_Join_Count(connectivity=w).fit(y_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.76 ms ± 178 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%t Join_Counts_Local_old(y_1, w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No apparent difference in speed?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bivariate Local Join Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/pysal/esda/blob/master/esda/lee.py\n",
    "import numpy\n",
    "from scipy import sparse\n",
    "from sklearn.base import BaseEstimator\n",
    "from sklearn import utils\n",
    "\n",
    "class Local_Join_Count_BV(BaseEstimator):\n",
    "    \"\"\"Global Spatial Pearson Statistic\"\"\"\n",
    "\n",
    "    def __init__(self, connectivity=None):\n",
    "        \"\"\"\n",
    "        Initialize a Join_Counts_Local estimator\n",
    "        Arguments\n",
    "        ---------\n",
    "        connectivity:   scipy.sparse matrix object\n",
    "                        the connectivity structure describing the relationships\n",
    "                        between observed units. Will be row-standardized. \n",
    "        Attributes\n",
    "        ----------\n",
    "        association_: numpy.ndarray (2,2)\n",
    "                      array containg the estimated Lee spatial pearson correlation\n",
    "                      coefficients, where element [0,1] is the spatial correlation\n",
    "                      coefficient, and elements [0,0] and [1,1] are the \"spatial\n",
    "                      smoothing factor\"\n",
    "        \"\"\"\n",
    "        \n",
    "        self.connectivity = connectivity\n",
    "\n",
    "    def fit(self, x, z):\n",
    "        \"\"\"\n",
    "        Arguments\n",
    "        ---------\n",
    "        y       :   numpy.ndarray\n",
    "                    array containing binary (0/1) data\n",
    "        Returns\n",
    "        -------\n",
    "        the fitted estimator.\n",
    "        Notes\n",
    "        -----\n",
    "        Technical details and derivations can be found in :cite:`Lee2001`.\n",
    "        \"\"\"\n",
    "        x = np.asarray(x).flatten()\n",
    "        z = np.asarray(z).flatten()\n",
    "        \n",
    "        w = self.connectivity\n",
    "        w.transformation = 'b' # Ensure we have binary weights   \n",
    "        \n",
    "        self.LJC_ = self._statistic(x, z, w) # Calculate the statistic\n",
    "        \n",
    "        # Need the >>> return self to get the associated .BB_ attribute (as well as significance in future, i.e. self.reference_distribution_ in lee.py)\n",
    "        return self\n",
    "        \n",
    "    @staticmethod\n",
    "    def _statistic(x, z, w):\n",
    "        adj_list = w.to_adjlist(remove_symmetric=False) # remove_symmetric=False differs from esda.Join_Counts() function\n",
    "        \n",
    "        # First, set up a series that maps the y values (input as self.y) to the weights table \n",
    "        zseries_x = pd.Series(x, index=w.id_order)\n",
    "        zseries_z = pd.Series(z, index=w.id_order)\n",
    "\n",
    "        # Next, map the y values to the focal (i) values \n",
    "        focal_x = zseries_x.loc[adj_list.focal].values\n",
    "        focal_z = zseries_z.loc[adj_list.focal].values\n",
    "\n",
    "        # Repeat the mapping but for the neighbor (j) values\n",
    "        neighbor_x = zseries_x.loc[adj_list.neighbor].values\n",
    "        neighbor_z = zseries_z.loc[adj_list.neighbor].values\n",
    "        \n",
    "        # Calculate Case 1\n",
    "        BJC = (focal_x == 1) & (focal_z == 0) & (neighbor_x == 0) & (neighbor_z == 1)\n",
    "        adj_list_BJC = pd.DataFrame(adj_list.focal.values, BJC.astype('uint8')).reset_index()\n",
    "        adj_list_BJC.columns = ['BJC', 'ID']\n",
    "        adj_list_BJC = adj_list_BJC.groupby(by='ID').sum()\n",
    "        \n",
    "        # Calculate Case 2\n",
    "        CLC = (focal_x == 1) & (focal_z == 1) & (neighbor_x == 1) & (neighbor_z == 1)\n",
    "        adj_list_CLC = pd.DataFrame(adj_list.focal.values, CLC.astype('uint8')).reset_index()\n",
    "        adj_list_CLC.columns = ['CLC', 'ID']\n",
    "        adj_list_CLC = adj_list_CLC.groupby(by='ID').sum()\n",
    "        \n",
    "        # Return values\n",
    "        return (adj_list_BJC.BJC.values, adj_list_CLC.CLC.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/pysal/esda/blob/master/esda/lee.py\n",
    "import numpy\n",
    "from scipy import sparse\n",
    "from sklearn.base import BaseEstimator\n",
    "from sklearn import utils\n",
    "\n",
    "class Local_Join_Count_BV_v2(BaseEstimator):\n",
    "    \"\"\"Global Spatial Pearson Statistic\"\"\"\n",
    "\n",
    "    def __init__(self, connectivity=None):\n",
    "        \"\"\"\n",
    "        Initialize a Join_Counts_Local estimator\n",
    "        Arguments\n",
    "        ---------\n",
    "        connectivity:   scipy.sparse matrix object\n",
    "                        the connectivity structure describing the relationships\n",
    "                        between observed units. Will be row-standardized. \n",
    "        Attributes\n",
    "        ----------\n",
    "        association_: numpy.ndarray (2,2)\n",
    "                      array containg the estimated Lee spatial pearson correlation\n",
    "                      coefficients, where element [0,1] is the spatial correlation\n",
    "                      coefficient, and elements [0,0] and [1,1] are the \"spatial\n",
    "                      smoothing factor\"\n",
    "        \"\"\"\n",
    "        \n",
    "        self.connectivity = connectivity\n",
    "\n",
    "    def fit(self, x, z, case=None):\n",
    "        \"\"\"\n",
    "        Arguments\n",
    "        ---------\n",
    "        y       :   numpy.ndarray\n",
    "                    array containing binary (0/1) data\n",
    "        Returns\n",
    "        -------\n",
    "        the fitted estimator.\n",
    "        Notes\n",
    "        -----\n",
    "        Technical details and derivations can be found in :cite:`Lee2001`.\n",
    "        \"\"\"\n",
    "        x = np.asarray(x).flatten()\n",
    "        z = np.asarray(z).flatten()\n",
    "        \n",
    "        w = self.connectivity\n",
    "        w.transformation = 'b' # Ensure we have binary weights   \n",
    "                \n",
    "        self.LJC_ = self._statistic(x, z, w, case=case) # Calculate the statistic\n",
    "        \n",
    "        return self\n",
    "        \n",
    "    @staticmethod\n",
    "    def _statistic(x, z, w, case=None):        \n",
    "        adj_list = w.to_adjlist(remove_symmetric=False) # remove_symmetric=False differs from esda.Join_Counts() function\n",
    "        \n",
    "        # First, set up a series that maps the y values (input as self.y) to the weights table \n",
    "        zseries_x = pd.Series(x, index=w.id_order)\n",
    "        zseries_z = pd.Series(z, index=w.id_order)\n",
    "\n",
    "        # Next, map the y values to the focal (i) values \n",
    "        focal_x = zseries_x.loc[adj_list.focal].values\n",
    "        focal_z = zseries_z.loc[adj_list.focal].values\n",
    "\n",
    "        # Repeat the mapping but for the neighbor (j) values\n",
    "        neighbor_x = zseries_x.loc[adj_list.neighbor].values\n",
    "        neighbor_z = zseries_z.loc[adj_list.neighbor].values\n",
    "        \n",
    "        if case==\"BJC\":\n",
    "            BJC = (focal_x == 1) & (focal_z == 0) & (neighbor_x == 0) & (neighbor_z == 1)\n",
    "            adj_list_BJC = pd.DataFrame(adj_list.focal.values, BJC.astype('uint8')).reset_index()\n",
    "            adj_list_BJC.columns = ['BJC', 'ID']\n",
    "            adj_list_BJC = adj_list_BJC.groupby(by='ID').sum()\n",
    "            return adj_list_BJC.BJC.values\n",
    "        elif case==\"CLC\": \n",
    "            CLC = (focal_x == 1) & (focal_z == 1) & (neighbor_x == 1) & (neighbor_z == 1)\n",
    "            adj_list_CLC = pd.DataFrame(adj_list.focal.values, CLC.astype('uint8')).reset_index()\n",
    "            adj_list_CLC.columns = ['CLC', 'ID']\n",
    "            adj_list_CLC = adj_list_CLC.groupby(by='ID').sum()\n",
    "            return (adj_list_CLC.CLC.values)\n",
    "        else:\n",
    "            print(\"Please specify which type of bivariate Local Join Count you would like to calculate (either 'BJC' or 'CLC'). See Anselin and Li 2019 p. 9-10 for more information\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test some values..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x [0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "z [0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "x = y_1\n",
    "z = [0,1,0,1,1,1,1,1,0,0,1,1,0,0,1,1]\n",
    "\n",
    "print('x', x)\n",
    "print('z', z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0], dtype=uint64),\n",
       " array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 0, 0, 2, 2], dtype=uint64))"
      ]
     },
     "execution_count": 272,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp = Local_Join_Count_BV(connectivity=w).fit(x,z)\n",
    "temp.LJC_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0]\n",
      "[0 0 0 0 0 0 0 0 0 0 2 2 0 0 2 2]\n"
     ]
    }
   ],
   "source": [
    "# Case 1\n",
    "temp2 = Local_Join_Count_BV_v2(connectivity=w).fit(x,z, case=\"BJC\")\n",
    "print(temp2.LJC_)\n",
    "# Case 2\n",
    "temp2 = Local_Join_Count_BV_v2(connectivity=w).fit(x,z, case=\"CLC\")\n",
    "print(temp2.LJC_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please specify which type of bivariate Local Join Count you would like to calculate (either 'BJC' or 'CLC'). See Anselin and Li 2019 p. 9-10 for more information\n",
      "Local_Join_Count_BV_v2(connectivity=<libpysal.weights.weights.W object at 0x1BA20628>)\n",
      "Please specify which type of bivariate Local Join Count you would like to calculate (either 'BJC' or 'CLC'). See Anselin and Li 2019 p. 9-10 for more information\n",
      "Local_Join_Count_BV_v2(connectivity=<libpysal.weights.weights.W object at 0x1BA20628>)\n"
     ]
    }
   ],
   "source": [
    "# Try with a purposefully wrong input or blnak\n",
    "# Improper input\n",
    "print(Local_Join_Count_BV_v2(connectivity=w).fit(x,z, case=\"ThisIsWrong\"))\n",
    "# No input for case\n",
    "print(Local_Join_Count_BV_v2(connectivity=w).fit(x,z))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multivariate Local Join Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/pysal/esda/blob/master/esda/lee.py\n",
    "import numpy\n",
    "from scipy import sparse\n",
    "from sklearn.base import BaseEstimator\n",
    "from sklearn import utils\n",
    "\n",
    "class Local_Join_Count_MV(BaseEstimator):\n",
    "    \"\"\"Global Spatial Pearson Statistic\"\"\"\n",
    "\n",
    "    def __init__(self, connectivity=None):\n",
    "        \"\"\"\n",
    "        Initialize a Join_Counts_Local estimator\n",
    "        Arguments\n",
    "        ---------\n",
    "        connectivity:   scipy.sparse matrix object\n",
    "                        the connectivity structure describing the relationships\n",
    "                        between observed units. Will be row-standardized. \n",
    "        Attributes\n",
    "        ----------\n",
    "        association_: numpy.ndarray (2,2)\n",
    "                      array containg the estimated Lee spatial pearson correlation\n",
    "                      coefficients, where element [0,1] is the spatial correlation\n",
    "                      coefficient, and elements [0,0] and [1,1] are the \"spatial\n",
    "                      smoothing factor\"\n",
    "        \"\"\"\n",
    "        \n",
    "        self.connectivity = connectivity\n",
    "\n",
    "    def fit(self, variables):\n",
    "        \"\"\"\n",
    "        Arguments\n",
    "        ---------\n",
    "        y       :   numpy.ndarray\n",
    "                    array containing binary (0/1) data\n",
    "        Returns\n",
    "        -------\n",
    "        the fitted estimator.\n",
    "        Notes\n",
    "        -----\n",
    "        Technical details and derivations can be found in :cite:`Lee2001`.\n",
    "        \"\"\"\n",
    "            \n",
    "        # Need not be flattened?\n",
    "        \n",
    "        w = self.connectivity\n",
    "        w.transformation = 'b' # Ensure we have binary weights   \n",
    "                \n",
    "        self.MJC_ = self._statistic(variables, w) # Calculate the statistic\n",
    "        \n",
    "        return self\n",
    "        \n",
    "    @staticmethod\n",
    "    def _statistic(variables, w):\n",
    "        \n",
    "        adj_list = w.to_adjlist(remove_symmetric=False) # remove_symmetric=False differs from esda.Join_Counts() function\n",
    "        \n",
    "        # The zseries\n",
    "        zseries = [pd.Series(i, index=w.id_order) for i in variables]\n",
    "        # The focal values\n",
    "        focal = [zseries[i].loc[adj_list.focal].values for i in range(len(variables))]\n",
    "        # The neighbor values\n",
    "        neighbor = [zseries[i].loc[adj_list.neighbor].values for i in range(len(variables))]\n",
    "        \n",
    "        # Find instances where all surrounding focal and neighbor values == 1\n",
    "        focal_all = np.multiply(*focal)\n",
    "        neighbor_all = np.multiply(*neighbor)\n",
    "        MCLC = (focal_all == 1) & (neighbor_all == 1)\n",
    "\n",
    "        # Create a df that uses the adjacency list focal values and the BBs counts\n",
    "        adj_list_MCLC = pd.DataFrame(adj_list.focal.values, MCLC.astype('uint8')).reset_index()\n",
    "        # Temporarily rename the columns\n",
    "        adj_list_MCLC.columns = ['MCLC', 'ID']\n",
    "        adj_list_MCLC = adj_list_MCLC.groupby(by='ID').sum()\n",
    "        \n",
    "        return (adj_list_MCLC.MCLC.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x [0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1]\n",
      "z [0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1]\n",
      "y [0 1 1 1 1 1 1 1 0 0 0 1 0 0 1 1]\n"
     ]
    }
   ],
   "source": [
    "x = x.astype(np.int32)\n",
    "print('x', x)\n",
    "print('z', z)\n",
    "y = [0,1,1,1,1,1,1,1,0,0,0,1,0,0,1,1]\n",
    "y = np.asarray(y).flatten()\n",
    "print('y', y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 2], dtype=uint64)"
      ]
     },
     "execution_count": 323,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp = Local_Join_Count_MV(connectivity=w).fit([x,y,z])\n",
    "temp.MJC_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LOSH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/pysal/esda/blob/master/esda/lee.py\n",
    "import numpy\n",
    "from scipy import sparse\n",
    "from sklearn.base import BaseEstimator\n",
    "from sklearn import utils\n",
    "import pysal.lib as lp\n",
    "\n",
    "class LOSH(BaseEstimator):\n",
    "    \"\"\"Local Join Count Statistic\"\"\"\n",
    "\n",
    "    def __init__(self, connectivity=None):\n",
    "        \"\"\"\n",
    "        Initialize a Join_Counts_Local estimator\n",
    "        Arguments\n",
    "        ---------\n",
    "        connectivity:   scipy.sparse matrix object\n",
    "                        the connectivity structure describing the relationships\n",
    "                        between observed units. Will be row-standardized. \n",
    "        Attributes\n",
    "        ----------\n",
    "        BB_:  numpy.ndarray (1,)\n",
    "              array containing the estimated Local Join Count coefficients, \n",
    "              where element [0,0] is the number of Local Join Counts, ...\n",
    "        \"\"\"\n",
    "        \n",
    "        self.connectivity = connectivity\n",
    "\n",
    "    def fit(self, y):\n",
    "        \"\"\"\n",
    "        Arguments\n",
    "        ---------\n",
    "        y       :   numpy.ndarray\n",
    "                    array containing continuous data\n",
    "        Returns\n",
    "        -------\n",
    "        the fitted estimator.\n",
    "        Notes\n",
    "        -----\n",
    "        Technical details and derivations can be found in :cite:`OrdGetis2012`.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Define variable of interest\n",
    "        y = np.asarray(y).flatten()\n",
    "        \n",
    "        # Define weights of interest\n",
    "        w = self.connectivity\n",
    "        \n",
    "        # Row standardize the weights\n",
    "        w.transform = 'r' # Ensure we have binary weights   \n",
    "        \n",
    "        self.LOSH_ = self._statistic(y, w) # Calculate the statistic\n",
    "        \n",
    "        # Need the >>> return self to get the associated .BB_ attribute (as well as significance in future, i.e. self.reference_distribution_ in lee.py)\n",
    "        return self\n",
    "        \n",
    "    @staticmethod\n",
    "    def _statistic(y, w):\n",
    "        ylag = lp.weights.lag_spatial(w,y)\n",
    "        w_lens = [len(w[i]) for i in range(len(dict(w)))]\n",
    "        ymean = ylag/w_lens\n",
    "        yresid = y-ymean\n",
    "        # Scenario 1: a = 1, an absolute deivations measure $H_{i} = 1$\n",
    "        sc1 = (w_lens*(abs(yresid)**1))/w_lens\n",
    "        # Scenario 2: a = 2, a variance measure $H_{i} = 2$\n",
    "        sc2 = (w_lens*(abs(yresid)**2))/w_lens\n",
    "        return (sc1, sc2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test values based on existing Global Spatial Autocorrelation notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load modules\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import pysal.lib as lp\n",
    "import matplotlib.pyplot as plt\n",
    "import rasterio as rio\n",
    "import numpy as np\n",
    "import shapely.geometry as geom\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = gpd.read_file('C:/Users/jeffe/Dropbox/Maryland/PhD_Courses/GEOG788P/MnM4SDS_Fall2019/lectures/data/neighborhoods.gpkg')\n",
    "listings = gpd.read_file('C:/Users/jeffe/Dropbox/Maryland/PhD_Courses/GEOG788P/MnM4SDS_Fall2019/lectures/data/listings.gpkg')\n",
    "listings['price'] = listings.price.str.replace('$', '').str.replace(',','_').astype(float)\n",
    "median_price = gpd.sjoin(listings[['price', 'geometry']], df, op='within')\\\n",
    "                  .groupby('index_right').price.median()\n",
    "df['median_pri'] = median_price.values\n",
    "# Make sure missing values are taken care of\n",
    "pd.isnull(df['median_pri']).sum()\n",
    "df = df\n",
    "df['median_pri'].fillna((df['median_pri'].mean()), inplace=True)\n",
    "y = df['median_pri']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = lp.weights.Queen.from_dataframe(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pass through function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([111.66666667,  51.875     , 204.68      ,  92.22222222,\n",
       "         84.92      ,  22.22222222,  73.5       ,  88.75      ,\n",
       "        169.48      ,  44.        ,  83.11111111,  57.5       ,\n",
       "         46.22222222,  78.1875    , 161.08024691,  83.4609375 ,\n",
       "        120.        ,  70.15625   , 102.16666667, 159.26      ,\n",
       "         33.22222222, 100.08333333, 216.15277778,  82.86111111,\n",
       "        129.2265625 ,  70.32      ,  51.08      ,  87.5703125 ,\n",
       "         67.3       , 168.265625  ,  60.83333333, 100.421875  ,\n",
       "         64.08      , 302.81944444,  79.23611111,  84.55555556,\n",
       "        198.28      ,  98.3046875 ,  72.66326531,  98.16326531,\n",
       "         72.66      ,  65.55555556, 139.5       , 202.87755102]),\n",
       " array([12469.44444444,  2691.015625  , 41893.9024    ,  8504.9382716 ,\n",
       "         7211.4064    ,   493.82716049,  5402.25      ,  7876.5625    ,\n",
       "        28723.4704    ,  1936.        ,  6907.45679012,  3306.25      ,\n",
       "         2136.49382716,  6113.28515625, 25946.84594574,  6965.72808838,\n",
       "        14400.        ,  4921.89941406, 10438.02777778, 25363.7476    ,\n",
       "         1103.71604938, 10016.67361111, 46722.02334105,  6865.96373457,\n",
       "        16699.50445557,  4944.9024    ,  2609.1664    ,  7668.55963135,\n",
       "         4529.29      , 28313.32055664,  3700.69444444, 10084.55297852,\n",
       "         4106.2464    , 91699.61593364,  6278.36130401,  7149.64197531,\n",
       "        39314.9584    ,  9663.81158447,  5279.95012495,  9636.02665556,\n",
       "         5279.4756    ,  4297.5308642 , 19460.25      , 41159.30070804]))"
      ]
     },
     "execution_count": 341,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp = LOSH(connectivity=w).fit(y)\n",
    "temp.LOSH_"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
