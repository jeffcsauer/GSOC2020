# A conclusion to Google Summer of Code 2020

*A huge thank you to to @sjsrey, @ljwolf, @darribas, @slumnitz, @TaylorOshan, and the larger [PySAL group](https://github.com/orgs/pysal/people) for acting as mentors throughout the summer and welcoming me to the PySAL community. This was a wonderful exercise in developing open-source software and I would highly recommend future students consider contributing to PySAL!*

After a summer condensed into ten blog posts we arrive at the finale - one final blog post to mark the end of the program. If you are here for the first time, a short re-introduction of the project is worthwhile. The Google Summer of Code (GSOC) program connects students with open-source software development groups. Before making a formal application to the program, applications seek out a development group relevant to their interests - in my case, quantitative geography - and start to contribute in small ways to the existing software. Some students make sizable contributions before the program even begins! Once the student feels that the software development group is a good fit, the student prepares a proposal. The proposal includes a week-by-week breakdown of their expected contributions to the software, split into Phases I, II, and III. As you might imagine, we are now at the end of Phase III. 

My [proposal](https://docs.google.com/document/d/1WjHjy5Eyk4WG5QWfnsnhWg1r4-e09JXXCx0iaPphg6c/edit?usp=sharing) sought to add several newly(ish) identified spatial statistics to the Exploratory Spatial Data Analysis (ESDA) submodule of PySAL. These spatial statistics include Local Join Counts (univariate, bivariate, and multivariate) proposed by [Anselin and Li (2019)](https://link.springer.com/article/10.1007/s10109-019-00299-x), as well as Local Spatial Heteroskedasticity (LOSH) proposed by [Ord and Getis (2012)](https://link.springer.com/article/10.1007/s00168-011-0492-y). This brought the total number of functions to be implemented to four. As with the other functions available in ESDA and throughout PySAL, each of these functions would include docstrings, doctests, and documentation so that future users would have ample help files should they encounter any issues. 

My general strategy towards implementation was as follows

1. Read the original papers and work through a manual calculation of the spatial statistic. 
2. Transition the knowledge gained in Step 1 to a Jupyter Notebook. These notebooks are available [here](https://github.com/jeffcsauer/GSOC2020/tree/master/validation) and include detailed step-by-step calculations of the statistics on a 'toy' dataset.
3. Translate the step-by-step calculations into a draft function. The initial drafts of each function were written as standard Python functions. When available, results of the function were compared to existing implementations in R (for LOSH) or C++ (for Local Join Counts).

*Get feedback from mentors...*

4. Transform the standard form Python functions into Scikit-learn style functions.
5. Validate the results of the function across several variables and datasets using existing implementations available in R or C++.

*Get more feedback from mentors!*

6. Let the functions 'settle' for a bit so any bugs can arise naturally. This was an important step as some issues would not make themselves known until some time had past after the initial implementation. 
7. Fill out docstrings, doctests, and documentation notebooks, and write continuous integration tests. 
8. Open a pull request on `pysal/esda`!

Although I would often be able to get quick versions of the functions up and running, it was often the fine-tuning of the functions that took the most time. One clear example is the implementation of statistical inference across several of the functions. The Local Join Count functions depend on conditional randomization to determine significance of a given area unit. Initially, I implemented inference following a version of the conditional randomization engine in PySAL called `_crand()`. As timing would have it, PySAL contributors Levi and Dani were working on a new version of this engine that utilized `numba`, which would allow for much faster conditional randomization. Although inference is only one part of the Local Join Count statistics, a bulk of the time was spent coding inference for both versions of the engine. The real heroes of this side story are Levi and Dani, whose [`numba`-ized conditional randomization engine](https://github.com/pysal/esda/blob/master/esda/crand.py) offers the same inference in a fraction of the previous time. 

# Stretch goals - even more functions!

Towards the mid-to-end of July (and Phase II), significant progress had been made on the original goals of the project. All of the four initial functions had been written, and so I worked with the project mentors to outline additional goals to be completed by the end of GSOC. These so-called 'stretch goals' focused on implementing another set of spatial estimators called local Geary statistics. Unlike the LOSH and Local Join Count estimators, the local Geary statistics have existed for quite some time in the spatial analysis literature. The univariate version of the local Geary statistic was outlined by [Anselin in 1995](https://onlinelibrary.wiley.com/doi/abs/10.1111/j.1538-4632.1995.tb00338.x), and expanded upon more than 20 years later by [Anselin in 2017](https://geodacenter.github.io/docs/LA_multivariateGeary1.pdf#10). 

It was with the multivariate local Geary statistic where I encountered the most issue. The existing conditional randomization engines in PySAL focus on univariate and bivariate inputs (ex. `Local_Moran` versus `Local_Moran_BV`). However, none of the statistics handle multivariate inputs. While I could use the existing setup of the [`_crand()` engine](https://github.com/jeffcsauer/GSOC2020/blob/master/functions/local_geary_mv.py#L131-L142), the actual subsetting of multivariate inputs and calculation had to be modified quite a bit to handle inputs of 2 or more variables. Luckily, after a couple days of tinkering around I figured out a relatively straightforward subsetting solution that should have occurred to me much sooner! It was a great moment of *think smarter, not harder*. In short, the new solution first holds area *i* in place, randomizes the rest of the areas, subsets the random neighbors of area *i*, and carries out the local Geary calculation. All in all, I was able to add six functions to the ESDA submodule! 

# Concluding thoughts

This summer has proven immensely rewarding. There are a few key reflections I would like to share as takeaway points:

**Reading a paper is different from understanding a paper, and implementing a paper is different from understanding a paper.**

As explained above in my implementation strategy, my first step in implementation was always to read the original paper. After a few reads, I would come away from the paper thinking that I had a good understanding of what was being proposed. Yet when I would begin to work through a manual calculation of the statistic I often revealed to myself something I had missed in the paper. Returning to the paper, that previously overlooked detail was resting their plainly among the other sentences and words! This process repeated itself when transitioning from the manual calculations to the implementation in code, as more and more details revealed themselves on the original paper. Sentences I glossed over on the first read are now emphasized by my eyes, as they hold the secret to a tricky addition, for loop, or row sum. I look back at the original papers with an entirely new atmosphere of appreciation and reverence. 

**Software development can advance science, even if the software development itself may not always be science.** 

In my opinion, there is a difference between the idea of a statistic, how it is brought to life through code, and the scientific problem it might ultimately address. When I was piecing together how to make a function like `losh` producing the right numbers, I did not feel as if I was 'doing science' in that moment (e.g. figuring out a for loop or the exact behavior of indexing on weights). Yet when the function began producing the right numbers, and when I started preparing documentation to be shared with future users, I did feel that I was 'doing science'. 

**Reading code is a lot like reading a book. With every additional file opened, you realize there is so much more you have to learn.**

While I read a book, I often have the thought that the page-turning motion is an endless one. With each page completed, you turn over to the next page. One could steadily repeat this process their entire life and never turn all the pages in the world. They would likely not get even close. Indeed, with every turn of the page we move further away from where we started. I have started to have this feeling with code. As my skills progress (albeit meagerly), I find myself curiouser and curiouser about the coding of others. I open up files, peek into repositories, and try to read languages that I haven't yet even installed on my computer. I don't understand most of it, but does anyone understand each page of a book? Every sentence and phrase the author put into place? 

All in all, I am so very thankful for the opportunity to work with PySAL, NumFOCUS, and Google during the summer of 2020. This will certainly be a summer to remember.
